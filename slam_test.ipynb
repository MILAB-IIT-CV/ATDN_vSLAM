{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing odometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from slam_framework.neural_slam import NeuralSLAM\n",
    "from helpers import Arguments, log\n",
    "from odometry.vo_datasets import KittiOdometryDataset\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "\n",
    "\n",
    "args = Arguments.get_arguments()\n",
    "sequence_length = 1\n",
    "preprocessed_flow = True\n",
    "\n",
    "dataset = KittiOdometryDataset(args.data_path, \"00\", precomputed_flow=preprocessed_flow, sequence_length=sequence_length)\n",
    "slam = NeuralSLAM(args, preprocessed_flow=preprocessed_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slam.start_odometry()\n",
    "print(\"SLAM mode: \", slam.mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_scale = []\n",
    "count = 0\n",
    "slam_call_time = []\n",
    "for i in range(0, len(dataset), sequence_length):\n",
    "    if preprocessed_flow:\n",
    "        img1, img2, flow, _, __ = dataset[i]\n",
    "        start = time.time()\n",
    "        current_pose = slam(img1.squeeze(), img2.squeeze(), flow.squeeze())\n",
    "        end = time.time()\n",
    "    else:\n",
    "        img1, img2, _, __ = dataset[i]\n",
    "        start = time.time()\n",
    "        current_pose = slam(img1.squeeze(), img2.squeeze())\n",
    "        end = time.time()\n",
    "        \n",
    "    slam_call_time.append(end-start)\n",
    "    global_scale += current_pose\n",
    "    \n",
    "    print('    ', end='\\r')\n",
    "    print(count, end='')\n",
    "    count = count+sequence_length\n",
    "\n",
    "global_scale = torch.stack(global_scale, dim=0)\n",
    "slam_call_time = np.array(slam_call_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPS calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy time\n",
    "log(\"Average odometry time: \", slam_call_time.mean())\n",
    "log(\"Odometry time std: \", slam_call_time.std())\n",
    "\n",
    "# FPS manual calc\n",
    "fps_manual = 1/(slam_call_time.mean())\n",
    "log(\"FPS from time: \", 1/slam_call_time.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyframe check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slam.end_odometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "DATA_PATH = args.keyframes_path + \"/poses.pth\"\n",
    "poses = torch.load(DATA_PATH)\n",
    "print(poses.shape)\n",
    "X = poses[:, 3]\n",
    "Z = poses[:, -1]\n",
    "plt.scatter(X.numpy(), Z.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyframe_poses = []\n",
    "for i in range(len(slam)):\n",
    "    keyframe_poses.append(slam.get_keyframe(i).pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyframe_positions = []\n",
    "for pose in keyframe_poses:\n",
    "\n",
    "    keyframe_positions.append(pose[:-1, -1])\n",
    "\n",
    "keyframe_positions = torch.stack(keyframe_positions, dim=0)\n",
    "print(keyframe_positions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib as mpl\n",
    "#mpl.rcParams['figure.dpi'] = 150\n",
    "\n",
    "global_pos = global_scale[:, :3, -1]\n",
    "X, Y, Z = global_pos[:, 0], global_pos[:, 1], global_pos[:, 2]\n",
    "X_key, Y_key, Z_key = keyframe_positions[:, 0], keyframe_positions[:, 1], keyframe_positions[:, 2]\n",
    "\n",
    "plt.plot(X, Z)\n",
    "#plt.plot(X[:100], Z[:100])\n",
    "plt.scatter(X_key, Z_key)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(Y)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from slam_framework.neural_slam import NeuralSLAM\n",
    "from helpers import Arguments, log\n",
    "from odometry.vo_datasets import KittiOdometryDataset\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "\n",
    "args = Arguments()\n",
    "sequence_length = 1\n",
    "preprocessed_flow = True\n",
    "DATA_PATH = None # TODO Change to config file data read\n",
    "slam = NeuralSLAM(DATA_PATH, preprocessed_flow=preprocessed_flow, start_mode=\"relocalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from localization.localization_dataset import MappingDataset, KittiLocalizationDataset\n",
    "from GMA.core.utils.utils import InputPadder\n",
    "from helpers import log, matrix2euler\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "\n",
    "dataset = MappingDataset(args.keyframes_path, slam=True)\n",
    "rgb_mean = torch.load(\"normalization_cache/rgb_mean.pth\").unsqueeze(-1).unsqueeze(-1).unsqueeze(0)\n",
    "rgb_std = torch.load(\"normalization_cache/rgb_std.pth\").unsqueeze(-1).unsqueeze(-1).unsqueeze(0)\n",
    "\n",
    "dataset = KittiLocalizationDataset(data_path=args.data_path, sequence=\"00\")\n",
    "\n",
    "rgb, true_orientation, true_position = dataset[195]\n",
    "\n",
    "padder = InputPadder(rgb.shape)\n",
    "rgb = padder.pad(rgb)[0]\n",
    "\n",
    "im_normalized = (rgb.unsqueeze(0)-rgb_mean)/rgb_std\n",
    "\n",
    "with torch.no_grad():\n",
    "    initial_pose, refined_pose, distances = slam(im_normalized)\n",
    "    #log(\"Distances shape\", distances.shape)\n",
    "\n",
    "    max_dist = torch.max(distances).cpu().numpy()\n",
    "    bins = 1000\n",
    "\n",
    "    [hist, bin_edges] = np.histogram(distances.cpu().numpy(), bins=bins)\n",
    "\n",
    "    # ---------\n",
    "    # Histogram\n",
    "    # ---------\n",
    "\n",
    "    plt.bar(bin_edges[:-1], hist, width=5)\n",
    "    plt.xlabel(\"Distance from sample\")\n",
    "    plt.ylabel(\"Count of elements\")\n",
    "    plt.show()\n",
    "\n",
    "    # Predicted index\n",
    "    distances_mean = distances.mean()\n",
    "    pred_index = torch.argmin(distances)\n",
    "    second_pred_index = torch.argmin(distances)\n",
    "\n",
    "    plt.plot(distances.cpu().numpy())\n",
    "    plt.xlabel(\"Index of keyframe\")\n",
    "    plt.ylabel(\"Embedding distance from sample\")\n",
    "    plt.show()\n",
    "\n",
    "def prepare_im(im):\n",
    "    return im.detach().byte().squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "\n",
    "pred_im, _, _ = dataset[int(pred_index.squeeze())]\n",
    "second_pred_im, _, _ = dataset[int(second_pred_index.squeeze())]\n",
    "\n",
    "def to_vectors(mat):\n",
    "    abs_rotation = mat[:3, :3]\n",
    "    abs_translation = mat[:3, -1]\n",
    "\n",
    "    orientation = matrix2euler(abs_rotation)\n",
    "    position = abs_translation\n",
    "    return orientation, position\n",
    "\n",
    "initial_orientation, initial_position = to_vectors(initial_pose)\n",
    "refined_orientation, refined_position = to_vectors(refined_pose)\n",
    "log(\"True pose: \", [true_orientation, true_position])\n",
    "log(\"Initial estimate: \", [initial_orientation, initial_position])\n",
    "log(\"Refined estimate: \", [refined_orientation, refined_position])\n",
    "\n",
    "log(\"Initial difference: \", [(true_orientation-initial_orientation).abs().sum(), (true_position-initial_position).abs().sum()])\n",
    "log(\"Refined difference: \", [(true_orientation-refined_orientation).abs().sum(), (true_position-refined_position).abs().sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = args.data_path + \"/dataset/poses/00.txt\"\n",
    "true_pos = np.loadtxt(DATA_PATH)\n",
    "\n",
    "X_gt, Y_gt, Z_gt = true_pos[:, 3], true_pos[:, 7], true_pos[:, 11]\n",
    "\n",
    "index = 2400\n",
    "diff = 20\n",
    "plt.plot(X_gt, Z_gt)\n",
    "plt.plot(X_gt[index-diff:index+diff], Z_gt[index-diff:index+diff])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing reconstruction of keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = []\n",
    "for i in range(len(slam)):\n",
    "    poses.append(slam[i].pose.flatten())\n",
    "poses = torch.stack(poses, dim=0)\n",
    "\n",
    "X = poses[:, 3]\n",
    "Z = poses[:, 11]\n",
    "\n",
    "index = 195\n",
    "plt.scatter(X, Z)\n",
    "plt.scatter(X[index], Z[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing image from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = torch.load(slam[100].rgb_file_name)\n",
    "rgb_mean = torch.load(\"normalization_cache/rgb_mean.pth\").unsqueeze(-1).unsqueeze(-1).unsqueeze(0)\n",
    "rgb_std = torch.load(\"normalization_cache/rgb_std.pth\").unsqueeze(-1).unsqueeze(-1).unsqueeze(0)\n",
    "\n",
    "rgb = (rgb*rgb_std)+rgb_mean\n",
    "\n",
    "print(rgb.shape)\n",
    "plt.imshow(rgb.squeeze().byte().permute(1, 2, 0).numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d9c4e0ef343f48ddb8b3296df994136284bc2ec1cb5bd3ca0b75344a28b97fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
